{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         id  \\\n",
      "0  5dd50ed43a55ac51376178d1   \n",
      "1  5dd6604a3a55ac78684acf19   \n",
      "2  5e539eca3a55ac4db70a52b7   \n",
      "3  5e539eca3a55ac4db70a52d0   \n",
      "4  5e5794b791e0115453751069   \n",
      "\n",
      "                                             context  \\\n",
      "0  Recent advances in deep learning have focused ...   \n",
      "1  Current geolocalisation approaches require ima...   \n",
      "2  Formulating efficient SQL queries is a challen...   \n",
      "3  Two-sample tests are utilized to determine if ...   \n",
      "4  Bandit learning algorithms typically balance e...   \n",
      "\n",
      "                                            key_idea  \\\n",
      "0  The authors propose a metric, based on the Fis...   \n",
      "1  The authors propose a novel approach to geoloc...   \n",
      "2  The authors propose a new approach for predict...   \n",
      "3  The authors suggest a new kernel-based two-sam...   \n",
      "4  This paper proposes simple greedy algorithms f...   \n",
      "\n",
      "                                              method  \\\n",
      "0  The authors provide a theoretical analysis inc...   \n",
      "1  The model uses a sequence of captured panorami...   \n",
      "2  The authors employ data-driven machine learnin...   \n",
      "3  Deep neural networks are trained to maximize t...   \n",
      "4  A unified regret analysis for the proposed alg...   \n",
      "\n",
      "                                             outcome  \\\n",
      "0  The proposed method has been shown to successf...   \n",
      "1  Over 90% accuracy is achieved for geolocalisat...   \n",
      "2  Empirical results demonstrate that the predict...   \n",
      "3  The authors demonstrate the superior performan...   \n",
      "4  Sharper regret bounds were obtained compared t...   \n",
      "\n",
      "                                    projected_impact  \n",
      "0                                               None  \n",
      "1                                               None  \n",
      "2  The introduction of a data-driven machine lear...  \n",
      "3  The ideas and methods proposed in this paper h...  \n",
      "4  Our improved analysis proves that there is imp...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/jimmyzxj/massw/massw_data/train-00000-of-00001-30d85c6bc506170b.parquet\")\n",
    "# Inspect the data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine fields into a single prompt-response format with projected impact\n",
    "def format_data_with_impact(row):\n",
    "    return f\"\"\"\n",
    "    Context: {row['context']}\n",
    "    Key Idea: {row['key_idea']}\n",
    "    Method: {row['method']}\n",
    "    Outcome: {row['outcome']}\n",
    "    Question: Based on the above information, what is the projected impact?\n",
    "    Projected Impact: {row['projected_impact'] if row['projected_impact'] else 'Unknown'}\n",
    "    \"\"\"\n",
    "\n",
    "# Apply the formatting\n",
    "df[\"formatted\"] = df.apply(format_data_with_impact, axis=1)\n",
    "\n",
    "# Save for training\n",
    "df[\"formatted\"].to_csv(\"training_data_with_impact.txt\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/main/miniconda3/envs/palantir/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "/var/folders/ps/s5qr9q_n6n7c92gzllvdjqwh0000gn/T/ipykernel_20242/1238825850.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f\"{model_dir}/consolidated.00.pth\", map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define the paths\n",
    "model_dir = \"/Users/main/Documents/Data for palantir/Massw/models/original\"  # Path to your model folder\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Load the model and map it to the CPU\n",
    "try:\n",
    "    state_dict = torch.load(f\"{model_dir}/consolidated.00.pth\", map_location=torch.device(\"cpu\"))\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_dir, state_dict=state_dict)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    model = None\n",
    "\n",
    "if model:\n",
    "    # Test the model with a prompt\n",
    "    prompt = \"Explain advancements in AI.\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate a response\n",
    "    try:\n",
    "        outputs = model.generate(**inputs, max_length=100)\n",
    "        print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, PeftModel, PeftModelForCausalLM\n",
    "\n",
    "# Load the quantized model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "model_path = \"./models/Llama-3.2-3B-Instruct-Q4_K_M.gguf\"  # Update with your model file path\n",
    "llm = Llama(model_path=model_path)\n",
    "# Set up LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Fine-tune attention layers\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Prepare the model with LoRA\n",
    "model = PeftModel.from_pretrained(model, lora_config)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama_finetuned\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=1000,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Enable mixed precision training\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA fine-tuned model\n",
    "model.save_pretrained(\"./llama_finetuned\")\n",
    "tokenizer.save_pretrained(\"./llama_finetuned\")\n",
    "\n",
    "# Load the fine-tuned model for inference\n",
    "from transformers import pipeline\n",
    "\n",
    "fine_tuned_model = PeftModelForCausalLM.from_pretrained(\"./llama_finetuned\")\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)\n",
    "\n",
    "# Test inference\n",
    "prompt = \"Context: Advances in AI have focused on...\"\n",
    "result = pipe(prompt, max_length=100)\n",
    "print(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palantir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
